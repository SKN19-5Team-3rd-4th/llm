from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from typing import TypedDict, Annotated, Optional, Literal, List

from functools import partial
import operator
from dotenv import load_dotenv

from cache.dummydata import ModelCollect, ModelRecommend, ModelQna

load_dotenv()

# 정보 저장 state 선언 --------------------

class GraphState(TypedDict):
    messages: Annotated[list, add_messages]                         # 모든 메시지를 저장하는 리스트

    current_stage: Literal["collect", "recommend", "qna", "exit"]   # 현재 어떤 작업을 하고 있는지 저장

    collected_data: Optional[dict]                                  # 사용자에게서 모은 데이터(정보)를 저장하는 딕셔너리

    recommend_result: Annotated[Optional[List[str]], operator.add]  # 사용자에게 추천한 결과(해당 추천 결과는 재추천할때에 고려하지 않게 하기 위함)

    # None: 아무 행동도 하지 않음, Skip: 다음 단계로, Continue: 추천 만족, Retry: 추천 다시 받기, Restart: 처음부터 재시작, QnA: QnA로 이동
    user_action: Literal["None", "Skip", "Continue", "Retry", "Restart", "QnA", "Exit"]

    picture_exist: Optional[str]

initial_state = {
    "messages": [AIMessage(content="안녕하세요. AI입니다.")],
    "current_stage": "recommend",
    "user_action": "None",
    "collected_data": {
        "purpose": "거실 / 거실의 빈 벽이 너무 심심해서 놓고 싶어",
        "preferred_style": "미니멀",
        "preferred_color": "#A1B07C",
        "plant_type": '난',
        "season": "여름",
        "humidity": "건조",
        "has_dog": True,
        "has_cat": False,
        "isAirCond": True,
        "watering_frequency": "주1회 가능",
        "user_experience": "초보",
        "emotion": "위로",
    },
    "recommend_result": " "
}
### tools 선언 ---------------------------
# tool 함수 선언
""" 
@tool
def tool_func(들어갈 인자들(타입 힌트 포함)) -> str:
    # RAG 수행
    return string 
"""
# tools 에는, 각각 이미지 처리 혹은 RAG를 수행하는 세가지 함수가 들어가야 함
@tool
def tool_func(query: str) -> str:
    """
    이 함수는 테스트를 위한 값을 그대로 반환하는 함수입니다.
    반드시 해당 툴을 수행하세요.
    """
    return query

tools = [tool_func]

### 노드 선언 -----------------------------

def node_collect(state: GraphState, collector: ModelCollect):
    response, collected_data = collector.get_response(state["messages"], state["collected_data"])   # 어떤 정보를 전달했는지 알아야 하니까 collected_data도 같이 전달
    picture = collector.get_picture()   # 아마 get_picture에서 picture를 저장하고 해당 경로를 전달해야 할듯 (str)
    if picture is not None:
        return {
            "current_stage" : "collect",
            "messages": [response],                      # 사용자에게 보여줄 질문
            "picture_exist": picture,                    # 사진이 있다면 사진의 저장 경로
            "collected_data": collected_data,            # 업데이트된 새로운 정보 딕셔너리
        }
    else:
        return {
            "current_stage" : "collect",
            "messages": [response],
            "picture_exist": "None",
            "collected_data": collected_data,
        }

def node_recommend(state: GraphState, recommender: ModelRecommend):

    response, recommend_result = recommender.get_response(state["messages"], state["collected_data"], state["recommend_result"])  
    # collected_data (정보를 저장한 딕셔너리) 도 같이 전달해주는 것이 낫지 않을지...
    # 사용자에게 보여줘야할 값 : response와, 추천 결과: recommend_result를 같이 반환해줘야 할듯 (추천 결과는 다시 추천 받을때 제외하기 위함)
    # collected_data: dict         # 사용자에게서 모은 데이터(정보)를 저장하는 딕셔너리
    # recommend_result: List[str]  # 사용자에게 추천한 결과(해당 추천 결과는 재추천할때에 고려하지 않게 하기 위함)

    return {
        "current_stage" : "recommend",
        "messages": [response],
        "recommend_result": recommend_result,
    }

def node_qna(state: GraphState, chatbot: ModelQna):
    response = chatbot.get_response(state["messages"])

    return {
        "current_stage": "qna",
        "messages": [response],
    }

def node_end_state(state:GraphState):
    return {
        "current_stage": "exit"
    }


### router 선언 -----------------------

# 해당 router의 결과에 따라, 어떤 노드로 향할지 컨트롤
def main_router(state: GraphState):
    stage = state["current_stage"]
    action = state["user_action"]

    if action == "Restart":
        return "restart"
    
    if action == "Exit":
        return "exit"
    
    if stage == "collect":
        if ModelCollect.is_data_enough(state["collected_data"]):
            return "recommend"
        else:
            return "collect"
    
    elif stage == "recommend":
        if action == "Continue":
            return "exit"
        elif action == "QnA":
            return "qna"
        else:   # action == "Retry"
            return "recommend"
    
    elif stage == "qna":
        return "qna"
    
    elif stage == "exit":
        return "exit"
    
def is_tool_calls(state: GraphState):
    last_message = state["messages"][-1]

    if last_message.tool_calls:
        return "tool_call"
    else:
        return "done"
    
def tool_back_to_caller(state: GraphState) -> str:
    current_state = state["current_stage"]
    print('current_state: ', current_state)

    if current_state and current_state in ["collect", "recommend", "qna"]:
        return current_state
    
    return "exit"


### workflow 구현----------------------

model_collect = ModelCollect(tools)
model_recommend = ModelRecommend(tools)
model_qna = ModelQna(tools)

workflow = StateGraph(GraphState)

workflow.add_node("collect", partial(node_collect, collector=model_collect))
workflow.add_node("recommend", partial(node_recommend, recommender=model_recommend))
workflow.add_node("qna", partial(node_qna, chatbot=model_qna))
workflow.add_node("exit", node_end_state)
workflow.add_node("rag_tool", ToolNode(tools))

workflow.add_edge("exit", END)

workflow.add_conditional_edges(
    START,
    main_router,
    {
        "collect": "collect",
        "recommend": "recommend",
        "qna": "qna",
        "exit": "exit"
    }
)

workflow.add_conditional_edges(
    "collect",
    is_tool_calls,
    {
        "tool_call": "rag_tool",
        "done": END,
    }
)

workflow.add_conditional_edges(
    "recommend",
    is_tool_calls,
    {
        "tool_call": "rag_tool",
        "done": END,
    }
)

workflow.add_conditional_edges(
    "qna",
    is_tool_calls,
    {
        "tool_call": "rag_tool",
        "done": END,
    }
)

workflow.add_conditional_edges(
    "rag_tool",
    tool_back_to_caller,
    {
        "collect": "collect",
        "recommend": "recommend",
        "qna": "qna",
        "exit": "exit",
    }
)

### 그래프 컴파일 ----------------------
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

### 실제 구동 코드 ---------------------
def run_chat_loop(app, memory: MemorySaver, initial_state: dict):
    thread_id_01 = "basic_user"
    config = {"configurable": {"thread_id": thread_id_01}}

    response = app.invoke(initial_state, config=config)

    while True:
        current_state = response
        message = current_state["messages"][-1]

        print("="*30)
        print(f"채팅 시작: 현재 작업 {current_state['current_stage']}")
        print(f"AI   : {message.content}")
        print("="*30)
        if current_state["current_stage"] == "exit":
            print("종료합니다...")
            break
        user_input = input("User : ")
        action = "None"

        if user_input.lower() == "종료":
            action = "Exit"

        input_delta = {
            "messages": [HumanMessage(content=user_input)],
            "user_action": action,
        }

        response = app.invoke(input_delta, config=config)

### -----------------------------------

if __name__ == "__main__":
    run_chat_loop(app, memory, initial_state)

