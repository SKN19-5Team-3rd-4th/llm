{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b998ad",
   "metadata": {},
   "source": [
    "### 목표: RAG Q&A 동작 구축 → 그 다음에 LangGraph 연결 → 마지막에 Memory/ToolNode 확장\n",
    "\n",
    "#### 2단계: LangGraph 연결\n",
    "- 1단계에서 만든 RAG QnA를 Class로 정리\n",
    "- LangGraph Node로 연결 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47a0d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.tools import tool\n",
    "from functools import partial\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from typing import TypedDict, Annotated, Optional, Literal, List\n",
    "from dotenv import load_dotenv \n",
    "import operator\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"plant-qna\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "\n",
    "\n",
    "class ModelQna:\n",
    "    def __init__(self, llm, tools):\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self.prompt_template = \"\"\"\n",
    "        너는 식물에 대해 차분하게 상담해 주는 전문가이다.\n",
    "        아래 형식을 반드시 지키되, 실제 상담사가 말하듯 자연스럽고 단정적인 말투로 작성한다.\n",
    "        RAG 검색 정보를 참고하여 상담을 진행한다.\n",
    "        \n",
    "        ### RAG 검색 정보 ###\n",
    "        {context}\n",
    "\n",
    "        ### 답변 방식 ###\n",
    "        - 첫 문장은 사용자의 고민에 대한 핵심 답변을 한 줄로 요약한다. (채팅 응답처럼)\n",
    "        - 이후 이어지는 RAG 정보는 비슷한 사례의 해결 방향을 '요약 3줄'로 정리한다.\n",
    "        - 모든 문장은 따뜻하지만 과하지 않게, 실제 상담사가 말하듯 단정적으로 말한다.\n",
    "        - 마지막 문장은 대화를 이어가기 위해 질문형으로 마무리한다.\n",
    "        \n",
    "        ### 출력 형식 ###\n",
    "        [사용자의 상황을 판단해서 가장 핵심적인 조언을 한 문장으로 제시]\n",
    "        [현재 상황에 맞는 다음 추가 질문 유도]\n",
    "\n",
    "        사용자 질문: {question}\n",
    "        \"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(self.prompt_template)\n",
    "\n",
    "\n",
    "    def extract_question(self, messages):\n",
    "        human_msgs = [m.content for m in messages if isinstance(m, HumanMessage)]\n",
    "        return human_msgs[-1] if human_msgs else \"\"\n",
    "\n",
    "\n",
    "    def build_chain(self):\n",
    "        return (\n",
    "            {\n",
    "                \"context\": RunnableLambda(lambda q: next(t.run(q) for t in self.tools if t.name == \"tool_rag_qna\")),\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def get_response(self, messages):\n",
    "        q = self.extract_question(messages)\n",
    "        chain = self.build_chain()\n",
    "        response = chain.invoke(q)\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]   \n",
    "    current_stage: Literal[\"collect\", \"recommend\", \"qna\", \"exit\"]  \n",
    "    collected_data: Optional[dict]                               \n",
    "    recommend_result: Annotated[Optional[List[str]], operator.add]  \n",
    "    user_action: Literal[\"None\", \"Skip\", \"Continue\", \"Retry\", \"Restart\", \"QnA\",]\n",
    "    picture_exist: Optional[str]\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "\n",
    "@tool\n",
    "def tool_rag_qna(query: str) -> str:\n",
    "    \"\"\"식물 상담 QnA 전용 RAG 도구\"\"\"\n",
    "    \n",
    "    #----- 전역에서 한번만 바꿔야 할듯한\n",
    "    vector_store = PineconeVectorStore(index=index, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    ##-----\n",
    "\n",
    "    docs = retriever.invoke(query) # retriever(query)\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "tools = [tool_rag_qna]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def node_qna(state: GraphState, chatbot: ModelQna):\n",
    "    response = chatbot.get_response(state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": [response], # << 단일턴 | 멀티턴 필요하면 >> state[\"messages\"] + [response]\n",
    "        \"current_stage\": \"qna\",\n",
    "    }\n",
    "\n",
    "\n",
    "# 마지막 메시지에 tool_calls가 포함돼 있으면 tool_call 노드로 이동. 없으면 done으로 종료\n",
    "def is_tool_calls(state: GraphState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_call\"\n",
    "    else:\n",
    "        return \"done\"\n",
    "    \n",
    "\n",
    "# 툴이 실행된 후 다시 원래 단계(collect/recommend/qna) 로 돌아가도록\n",
    "def tool_back_to_caller(state: GraphState) -> str:\n",
    "    current_state = state.get(\"current_state\")\n",
    "\n",
    "    if current_state and current_state in [\"collect\", \"recommend\", \"qna\"]:\n",
    "        return current_state\n",
    "    \n",
    "    return \"done\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_qna = ModelQna(llm_with_tools)\n",
    "model_qna = ModelQna(llm_with_tools, tools)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"qna\", partial(node_qna, chatbot=model_qna))\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "workflow.add_conditional_edges(\"qna\", tools_condition)\n",
    "workflow.add_edge(\"tools\", \"qna\")\n",
    "workflow.add_edge(START, \"qna\")\n",
    "workflow.add_edge(\"qna\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef890a32",
   "metadata": {},
   "source": [
    "- 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf6ea967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"흙이 너무 축축한데 물을 줄까요?\")\n",
    "    ],\n",
    "    \"current_stage\": \"qna\",\n",
    "}\n",
    "\n",
    "result = app.invoke(test_state)\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5009665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='흙이 너무 축축한데 물을 줄까요?', additional_kwargs={}, response_metadata={}, id='b531886f-a3ef-4c32-a369-361ddd5ffcc4'),\n",
       "  HumanMessage(content='', additional_kwargs={}, response_metadata={}, id='142a1880-39f9-4435-8068-fb62b472f09a')],\n",
       " 'current_stage': 'qna'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "222db533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"산세베리아 잎이 노랗고 말랐어요. 물을 줘도 될까요?\")\n",
    "    ],\n",
    "    \"current_stage\": \"qna\",\n",
    "    \"collected_data\": None,\n",
    "    \"recommend_result\": [],\n",
    "    \"user_action\": \"None\",\n",
    "    \"picture_exist\": \"None\"\n",
    "}\n",
    "\n",
    "result = app.invoke(test_state)\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
