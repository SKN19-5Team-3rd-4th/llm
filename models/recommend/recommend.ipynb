{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import TypedDict, Annotated, Optional, Literal, List\n",
    "\n",
    "from functools import partial\n",
    "import operator\n",
    "\n",
    "from models.collect.collect import ModelCollect\n",
    "from models.recommend.recommend import ModelRecommend\n",
    "from models.qna.qna import ModelQna\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "# 정보 저장 state 선언 --------------------\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]                         # 모든 메시지를 저장하는 리스트\n",
    "\n",
    "    current_stage: Literal[\"collect\", \"recommend\", \"qna\", \"exit\"]   # 현재 어떤 작업을 하고 있는지 저장\n",
    "\n",
    "    collected_data: Optional[dict]                                  # 사용자에게서 모은 데이터(정보)를 저장하는 딕셔너리\n",
    "\n",
    "    recommend_result: Annotated[Optional[List[str]], operator.add]  # 사용자에게 추천한 결과(해당 추천 결과는 재추천할때에 고려하지 않게 하기 위함)\n",
    "\n",
    "    # None: 아무 행동도 하지 않음, Skip: 다음 단계로, Continue: 추천 만족, Retry: 추천 다시 받기, Restart: 처음부터 재시작, QnA: QnA로 이동\n",
    "    user_action: Literal[\"None\", \"Skip\", \"Continue\", \"Retry\", \"Restart\", \"QnA\",]\n",
    "\n",
    "    picture_exist: Optional[str]\n",
    "    \n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": []\n",
    "\n",
    "}\n",
    "### tools 선언 ---------------------------\n",
    "# tool 함수 선언\n",
    "\"\"\" \n",
    "@tools\n",
    "def tool_func(들어갈 인자들(타입 힌트 포함)) -> str:\n",
    "    # RAG 수행\n",
    "    return string \n",
    "\"\"\"\n",
    "\n",
    "# tools 에는, 각각 이미지 처리 혹은 RAG를 수행하는 세가지 함수가 들어가야 함\n",
    "tools = []\n",
    "\n",
    "### 노드 선언 -----------------------------\n",
    "\n",
    "def node_collect(state: GraphState, collector: ModelCollect):\n",
    "    response, collected_data = collector.get_response(state[\"messages\"], state[\"collected_data\"])   # 어떤 정보를 전달했는지 알아야 하니까 collected_data도 같이 전달\n",
    "    picture = collector.get_picture()   # 아마 get_picture에서 picture를 저장하고 해당 경로를 전달해야 할듯 (str)\n",
    "    if picture is not None:\n",
    "        return {\n",
    "            \"current_stage\" : \"collect\",\n",
    "            \"messages\": [response],  # 사용자에게 보여줄 질문\n",
    "            \"picture_exist\": picture,                   # 사진이 있다면 사진의 저장 경로\n",
    "            \"collected_data\": collected_data,            # 업데이트된 새로운 정보 딕셔너리\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"current_stage\" : \"collect\",\n",
    "            \"messages\": [response],\n",
    "            \"picture_exist\": \"None\",\n",
    "            \"collected_data\": collected_data,\n",
    "        }\n",
    "\n",
    "def node_recommend(state: GraphState, recommender: ModelRecommend):\n",
    "    response, recommend_result = recommender.get_response(state[\"messages\"], state[\"collected_data\"], state['recommend_result'])  \n",
    "    \n",
    "    # collected_data (정보를 저장한 딕셔너리) 도 같이 전달해주는 것이 낫지 않을지...   -> 어디에 전달하는 걸까요?\n",
    "    # 사용자에게 보여줘야할 값 : response와, 추천 결과: recommend_result를 같이 반환해줘야 할듯 (추천 결과는 다시 추천 받을때 제외하기 위함)\n",
    "    # collected_data: dict         # 사용자에게서 모은 데이터(정보)를 저장하는 딕셔너리\n",
    "    # recommend_result: List[str]  # 사용자에게 추천한 결과(해당 추천 결과는 재추천할때에 고려하지 않게 하기 위함)\n",
    "    return {\n",
    "        \"current_stage\" : \"recommend\",\n",
    "        \"messages\": [response],\n",
    "        \"recommend_result\": [recommend_result],\n",
    "    }\n",
    "\n",
    "def node_qna(state: GraphState, chatbot: ModelQna):\n",
    "    response = chatbot.get_response(state[\"messages\"])\n",
    "\n",
    "    return {\n",
    "        \"current_stage\": \"qna\",\n",
    "        \"messages\": [response],\n",
    "    }\n",
    "\n",
    "def node_end_state(state:GraphState):\n",
    "    return {\n",
    "        \"current_stage\": \"exit\"\n",
    "    }\n",
    "\n",
    "\n",
    "### router 선언 -----------------------\n",
    "\n",
    "# 해당 router의 결과에 따라, 어떤 노드로 향할지 컨트롤\n",
    "def main_router(state: GraphState):\n",
    "    stage = state[\"current_stage\"]\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    action = state[\"user_action\"]\n",
    "\n",
    "    if state[\"user_action\"] == \"Restart\":\n",
    "        return \"restart\"\n",
    "    \n",
    "    if stage == \"collect\":\n",
    "        if ModelCollect.is_data_enough(state[\"collected_data\"]):\n",
    "            return \"recommend\"\n",
    "        else:\n",
    "            return \"collect\"\n",
    "    \n",
    "    elif stage == \"recommend\":\n",
    "        if action == \"Continue\":\n",
    "            return \"exit\"\n",
    "        elif action == \"QnA\":\n",
    "            return \"qna\"\n",
    "        else:   # action == \"Retry\"\n",
    "            return \"recommend\"\n",
    "    \n",
    "    elif stage == \"qna\":\n",
    "        return \"qna\"\n",
    "    \n",
    "    elif stage == \"exit\":\n",
    "        return \"exit\"\n",
    "    \n",
    "def is_tool_calls(state: GraphState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_call\"\n",
    "    else:\n",
    "        return \"done\"\n",
    "    \n",
    "def tool_back_to_caller(state: GraphState) -> str:\n",
    "    current_state = state.get(\"current_state\")\n",
    "\n",
    "    if current_state and current_state in [\"collect\", \"recommend\", \"qna\"]:\n",
    "        return current_state\n",
    "    \n",
    "    return \"done\"\n",
    "\n",
    "\n",
    "### workflow 구현----------------------\n",
    "\n",
    "model_collect = ModelCollect(tools)\n",
    "model_recommend = ModelRecommend(tools)\n",
    "model_qna = ModelQna(tools)\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"collect\", partial(node_collect, collector=model_collect))\n",
    "workflow.add_node(\"recommend\", partial(node_collect, collector=model_recommend))\n",
    "workflow.add_node(\"qna\", partial(node_collect, collector=model_qna))\n",
    "workflow.add_node(\"exit\", node_end_state)\n",
    "workflow.add_node(\"rag_tool\", ToolNode(tools))\n",
    "\n",
    "workflow.set_entry_point(\"__start__\")\n",
    "\n",
    "workflow.add_edge(\"exit\", END)\n",
    "workflow.add_edge\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"__start__\",\n",
    "    main_router,\n",
    "    {\n",
    "        \"collect\": \"collect\",\n",
    "        \"recommend\": \"recommend\",\n",
    "        \"qna\": \"qna\",\n",
    "        \"done\": END,\n",
    "        \"exit\": \"exit\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"collect\",\n",
    "    is_tool_calls,\n",
    "    {\n",
    "        \"tool_call\": \"rag_tool\",\n",
    "        \"done\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"recommend\",\n",
    "    is_tool_calls,\n",
    "    {\n",
    "        \"tool_call\": \"rag_tool\",\n",
    "        \"done\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"qna\",\n",
    "    is_tool_calls,\n",
    "    {\n",
    "        \"tool_call\": \"rag_tool\",\n",
    "        \"done\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"rag_tool\",\n",
    "    tool_back_to_caller,\n",
    "    {\n",
    "        \"collect\": \"collect\",\n",
    "        \"recommend\": \"recommend\",\n",
    "        \"qna\": \"qna\",\n",
    "        \"exit\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "### 그래프 컴파일 ----------------------\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "### 실제 구동 코드 ---------------------\n",
    "def run_chat_loop(app, memory: MemorySaver, initial_state: dict):\n",
    "    thread_id_01 = \"basic_user\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id_01}}\n",
    "\n",
    "    response = app.invoke(initial_state)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            current_state = memory.get_state(config).values\n",
    "            message = current_state[\"messages\"][-1]\n",
    "\n",
    "            print(\"=\"*30)\n",
    "            print(f\"채팅 시작: 현재 작업 {current_state['current_stage']}\")\n",
    "            print(f\"AI   : {message}\")\n",
    "            print(\"=\"*30)\n",
    "            user_input = input(\"User : \")\n",
    "\n",
    "            if user_input.lower() == \"종료\":\n",
    "                print(\"종료합니다...\")\n",
    "                break\n",
    "\n",
    "            input_delta = {\n",
    "                \"messages\": [HumanMessage(content=user_input)],\n",
    "                \"user_action\": \"None\",\n",
    "            }\n",
    "\n",
    "            # 이후 확장성을 위해 app.stream 사용\n",
    "            stream_generator = app.stream(input_delta, config=config)\n",
    "\n",
    "            print(\"생성중\", end='')\n",
    "            for step in stream_generator:\n",
    "                print('.', end=' ', flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n",
    "            break\n",
    "\n",
    "### -----------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chat_loop(app, memory, initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa9a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tndl3\\anaconda3\\envs\\project3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d65e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_data = {\n",
    "    \"purpose\": \"거실 / 거실의 빈 벽이 너무 심심해서 놓고 싶어\",\n",
    "    \"preferred_style\": \"미니멀\",\n",
    "    \"preferred_color\": \"#A1B07C\",\n",
    "    \"plant_type\": '난',\n",
    "    \"season\": \"여름\",\n",
    "    \"humidity\": \"건조\",\n",
    "    \"has_dog\": True,\n",
    "    \"has_cat\": False,\n",
    "    \"isAirCond\": True,\n",
    "    \"watering_frequency\": \"주1회 가능\",\n",
    "    \"user_experience\": \"초보\",\n",
    "    \"emotion\": \"위로\",\n",
    "    \"description\":\"설명~\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. node_recommend()\n",
    "# 2. (rag 전) get_response(): tool 호출 \n",
    "# 3. tool recommend_rag(): 벡터DB에서 검색 후 반환\n",
    "# 4. (rag 후) get_response(): 검색된 데이터를 바탕으로 LLM이 응답 생성\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class ModelRecommend:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "\n",
    "    def get_response(self, messages, collected_data, prev_results: Optional[List[str]]):        \n",
    "        prompt= f\"\"\"\n",
    "            ### 요구사항 ###\n",
    "            {collected_data}\n",
    "\n",
    "            ### 이미 추천한 식물 (다시 추천하지 않습니다) ###\n",
    "            {prev_results}\n",
    "\n",
    "            ### 설명 ###\n",
    "            당신은 친절한 식물 전문가입니다. 한국어로 친절하게 답변하세요.\n",
    "            RAG 검색 결과를 참고해서 사용자에게 식물 1가지를 추천하세요.\n",
    "            이미 추천한 식물은 사용자가 거부한 식물이니 추천하지 않습니다.\n",
    "\n",
    "            ### 응답목록 ###\n",
    "            - 추천하는 식물 이름\n",
    "            - 추천하는 이유\n",
    "            - 식물의 꽃말\n",
    "            - 식물을 키우는 법\n",
    "            - 식물의 특징\n",
    "\n",
    "            ### 출력형식 ###\n",
    "            {{\n",
    "                \"flowNm\": \"\",\n",
    "                \"response\": \"\"\n",
    "            }}\n",
    "        \"\"\"\n",
    "        \n",
    "        system_msg = SystemMessage(prompt)\n",
    "        input_msg = [system_msg] + messages\n",
    "            \n",
    "        model = ChatOpenAI(\n",
    "            model='gpt-4o-mini',\n",
    "            temperature=1\n",
    "        ).bind_tools(self.tools)\n",
    "            \n",
    "        response = model.invoke(input_msg)\n",
    "        recommend_result = \"\"\n",
    "        if response.content != '':\n",
    "            res_json = json.loads(response.content)\n",
    "            recommend_result = res_json['flowNm']\n",
    "            \n",
    "        return response, recommend_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def tool_rag_qna(query: str) -> str:\n",
    "    \"\"\"식물 상담 QnA 전용 RAG 도구\"\"\"\n",
    "    \n",
    "    #----- 전역에서 한번만 바꿔야 할듯한\n",
    "\n",
    "    ##-----\n",
    "\n",
    "    docs = retriever.invoke(query) # retriever(query)\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "tools = [tool_rag_qna]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def tool_rag_recommend(query: str) -> str:\n",
    "    \"\"\"식물 추천 전용 RAG 도구\"\"\"\n",
    "    vector_store = PineconeVectorStore(\n",
    "        index=index, \n",
    "        embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    )\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "    retrievals = retriever.batch([query])\n",
    "    \n",
    "    return str(retrievals[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
